{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import math\nimport time\nimport torch\n\nimport yfinance as yf\nimport numpy as np\nimport pandas as pd\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nimport matplotlib.pyplot as plt\n\nfrom sklearn.preprocessing import StandardScaler, MinMaxScaler\nfrom torch.autograd import Variable\n\ntorch.manual_seed(1)","metadata":{"_uuid":"051d70d956493feee0c6d64651c6a088724dca2a","_execution_state":"idle"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def data_preprocessing(df):\n    df = df.fillna(method='ffill')\n    scaler = MinMaxScaler(feature_range=(-1, 1))\n    df['Close'] = scaler.fit_transform(df['Close'].values.reshape(-1, 1))\n    return df\n\ndef load_data(stock, look_back):\n    data_raw = stock.values\n    data = []\n    \n    # Split data into sequences (move 1 each time)\n    for index in range(len(data_raw)-look_back):\n        data.append(data_raw[index: index + look_back])\n    \n    data = np.array(data)\n    dt_size = data.shape[0]\n    test_set_size = int(np.round(0.2 * dt_size))\n    train_set_size = dt_size - (test_set_size)\n    \n    # Split by batch, for each sequence, choose the last as y\n    # Shape = [batches_num, lookback_len, 1]\n    x_train = data[:train_set_size, :-1, :]\n    y_train = data[:train_set_size, -1, :]\n    \n    x_test = data[train_set_size:, :-1, :]\n    y_test = data[train_set_size:, -1, :]\n    \n    return [x_train, y_train, x_test, y_test]","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_hsi = stock_data[['Close']]\ndf_hsi = data_preprocessing(df_hsi)\n\nlook_back = 60 # choose sequence length\nx_train, y_train, x_test, y_test = load_data(df_hsi, look_back)\n\n# make training and test sets in torch\nx_train = torch.from_numpy(x_train).type(torch.Tensor).to(device='cuda')\nx_test = torch.from_numpy(x_test).type(torch.Tensor).to(device='cuda')\ny_train = torch.from_numpy(y_train).type(torch.Tensor).to(device='cuda')\ny_test = torch.from_numpy(y_test).type(torch.Tensor).to(device='cuda')\n\nprint('x_train.shape = ', x_train.shape)\nprint('y_train.shape = ', y_train.shape)\nprint('x_test.shape = ', x_test.shape)\nprint('y_test.shape = ', y_test.shape)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class LSTM(nn.Module):\n    def __init__(self, input_dim, hidden_dim, num_layers, output_dim):\n        super(LSTM, self).__init__()\n        # Hidden dimensions\n        self.hidden_dim = hidden_dim\n        # Number of hidden layers\n        self.num_layers = num_layers\n        \n        # batch_first=True causes input/output tensors to be of shape\n        # (batch_dim, seq_dim, feature_dim)\n        self.lstm = nn.LSTM(input_dim, hidden_dim, num_layers, batch_first=True)\n        # Readout layer\n        self.fc = nn.Linear(hidden_dim, output_dim)\n        \n    def forward(self, x):\n        # Initialize hidden state with zeros\n        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_dim).requires_grad_().to(device='cuda')\n        # Initialize cell state\n        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_dim).requires_grad_().to(device='cuda')\n        # We need to detach as we are doing truncated backpropagation through time (BPTT)\n        # If we don't, we'll backprop all the way to the start even after going through another batch\n        out, (hn, cn) = self.lstm(x, (h0.detach(), c0.detach()))\n        \n        # Index hidden state of last time step\n        # out.size() --> 100, 32, 100\n        # out[:, -1, :] --> 100, 100 --> just want last time step hidden states! \n        out = self.fc(out[:, -1, :])\n        return out","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"input_dim = 1\nhidden_dim = 32\nnum_layers = 2 \noutput_dim = 1\n\nmodel = LSTM(input_dim=input_dim, hidden_dim=hidden_dim, output_dim=output_dim, num_layers=num_layers).cuda()\nloss_fn = torch.nn.MSELoss()\noptimiser = torch.optim.Adam(model.parameters(), lr=0.01)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(model)\nprint(len(list(model.parameters())))\nfor i in range(len(list(model.parameters()))):\n    print(list(model.parameters())[i].size())","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\"\"\"\n在pytorch里我算的是(正向)h_n[-2,:,:]=output[:,-1,:hiddensize], \n（反向）h_n[-1,:,:]=output[:,0,-hiddensize:]，\n如果单纯取output[:,-1,:]确实只包含了最后一个时间的输出，\n这对于正向是正确的，但对反向有问题。\n而正确的做法是取output[:,-1,:hiddensize]为正向最后一个时间的输出，\n以及output[:,0,-hiddensize:]为反向最后一个时间（即正向第一个时间）的输出\n\"\"\"\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Train model\n#####################\nnum_epochs = 100\nhist = np.zeros(num_epochs)\n\n# Number of steps to unroll\nseq_dim =look_back-1  \n\nfor t in range(num_epochs):\n    # Initialise hidden state\n    # Don't do this if you want your LSTM to be stateful\n    #model.hidden = model.init_hidden()\n    \n    # Forward pass\n    y_train_pred = model(x_train)\n\n    loss = loss_fn(y_train_pred, y_train)\n    if t % 10 == 0 and t !=0:\n        print(\"Epoch \", t, \"MSE: \", loss.item())\n    hist[t] = loss.item()\n\n    # Zero out gradient, else they will accumulate between epochs\n    optimiser.zero_grad()\n    \n    # Backward pass\n    loss.backward()\n\n    # Update parameters\n    optimiser.step()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.plot(hist, label=\"Training loss\")\nplt.legend()\nplt.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# make predictions\ny_test_pred = model(x_test)\n\n# invert predictions\ny_train_pred = scaler.inverse_transform(y_train_pred.detach().cpu().numpy())\ny_train = scaler.inverse_transform(y_train.detach().cpu().numpy())\ny_test_pred = scaler.inverse_transform(y_test_pred.detach().cpu().numpy())\ny_test = scaler.inverse_transform(y_test.detach().cpu().numpy())","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import mean_squared_error\n\n# calculate root mean squared error\ntrainScore = math.sqrt(mean_squared_error(y_train[:,0], y_train_pred[:,0]))\nprint('Train Score: %.2f RMSE' % (trainScore))\ntestScore = math.sqrt(mean_squared_error(y_test[:,0], y_test_pred[:,0]))\nprint('Test Score: %.2f RMSE' % (testScore))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Visualising the results\nfigure, axes = plt.subplots(figsize=(15, 6))\naxes.xaxis_date()\n\naxes.plot(df_hsi[len(df_hsi)-len(y_test):].index, y_test, color = 'red', label = 'Real HSI Stock Price')\naxes.plot(df_hsi[len(df_hsi)-len(y_test):].index, y_test_pred, color = 'blue', label = 'Predicted HSI Stock Price')\nplt.title('HSI Stock Price Prediction')\nplt.xlabel('Time')\nplt.ylabel('HSI Stock Price')\nplt.legend()\nplt.show()","metadata":{},"execution_count":null,"outputs":[]}]}